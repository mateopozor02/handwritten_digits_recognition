{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se importan los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1862b9877c0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#Imprime los tamanios de la matriz, y_train.shape() devuelve una sola componente pues son los resultados. \n",
    "print(X_train.shape, y_train.shape)\n",
    "plt.title(y_train[0])\n",
    "plt.imshow(X_train[0], cmap= 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesamiento de datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se normalizan las imagenes RBG diviendo entre 255\n",
    "X_train = X_train.astype(np.float32)/255 \n",
    "X_test = X_test.astype(np.float32)/255 \n",
    "\n",
    "#Se expande la dimension de las matrices para que el algoritmo pueda trabajar con ellas\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se convierten los vectores de resultados a matrices categoricas\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 50\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape= (28,28, 1)))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3, 3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "#La matriz de pooling se vuelve unidimensional (un vector)\n",
    "model.add(Flatten())\n",
    "\n",
    "#Previene overfitting\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#Fully connected output layer \n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,optimizer=keras.optimizers.Adadelta(),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Callbacks para monitorear overfitting\n",
    "from keras import callbacks\n",
    "\n",
    "es = callbacks.EarlyStopping(monitor= 'val_accuracy', min_delta= 0.01, patience= 4, verbose= 1)\n",
    "\n",
    "mc = callbacks.ModelCheckpoint(\"./mnist.h5\", monitor= 'val_accuracy', verbose= 1, save_best_only= True)\n",
    "\n",
    "cb = [es, mc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "469/469 [==============================] - ETA: 0s - loss: 2.2953 - accuracy: 0.1208\n",
      "Epoch 1: val_accuracy improved from -inf to 0.16690, saving model to .\\mnist.h5\n",
      "469/469 [==============================] - 39s 83ms/step - loss: 2.2953 - accuracy: 0.1208 - val_loss: 2.2768 - val_accuracy: 0.1669\n",
      "Epoch 2/50\n",
      "469/469 [==============================] - ETA: 0s - loss: 2.2650 - accuracy: 0.1774\n",
      "Epoch 2: val_accuracy improved from 0.16690 to 0.27730, saving model to .\\mnist.h5\n",
      "469/469 [==============================] - 39s 84ms/step - loss: 2.2650 - accuracy: 0.1774 - val_loss: 2.2443 - val_accuracy: 0.2773\n",
      "Epoch 3/50\n",
      "469/469 [==============================] - ETA: 0s - loss: 2.2314 - accuracy: 0.2510\n",
      "Epoch 3: val_accuracy improved from 0.27730 to 0.41300, saving model to .\\mnist.h5\n",
      "469/469 [==============================] - 41s 87ms/step - loss: 2.2314 - accuracy: 0.2510 - val_loss: 2.2071 - val_accuracy: 0.4130\n",
      "Epoch 4/50\n",
      "469/469 [==============================] - ETA: 0s - loss: 2.1920 - accuracy: 0.3271\n",
      "Epoch 4: val_accuracy improved from 0.41300 to 0.50050, saving model to .\\mnist.h5\n",
      "469/469 [==============================] - 41s 88ms/step - loss: 2.1920 - accuracy: 0.3271 - val_loss: 2.1624 - val_accuracy: 0.5005\n",
      "Epoch 5/50\n",
      "469/469 [==============================] - ETA: 0s - loss: 2.1440 - accuracy: 0.3983\n",
      "Epoch 5: val_accuracy improved from 0.50050 to 0.57040, saving model to .\\mnist.h5\n",
      "469/469 [==============================] - 41s 87ms/step - loss: 2.1440 - accuracy: 0.3983 - val_loss: 2.1070 - val_accuracy: 0.5704\n",
      "Epoch 6/50\n",
      "469/469 [==============================] - ETA: 0s - loss: 2.0835 - accuracy: 0.4643\n",
      "Epoch 6: val_accuracy improved from 0.57040 to 0.62410, saving model to .\\mnist.h5\n",
      "469/469 [==============================] - 41s 87ms/step - loss: 2.0835 - accuracy: 0.4643 - val_loss: 2.0372 - val_accuracy: 0.6241\n",
      "Epoch 7/50\n",
      "469/469 [==============================] - ETA: 0s - loss: 2.0074 - accuracy: 0.5196\n",
      "Epoch 7: val_accuracy improved from 0.62410 to 0.67350, saving model to .\\mnist.h5\n",
      "469/469 [==============================] - 41s 88ms/step - loss: 2.0074 - accuracy: 0.5196 - val_loss: 1.9487 - val_accuracy: 0.6735\n",
      "Epoch 8/50\n",
      "469/469 [==============================] - ETA: 0s - loss: 1.9137 - accuracy: 0.5712\n",
      "Epoch 8: val_accuracy improved from 0.67350 to 0.71170, saving model to .\\mnist.h5\n",
      "469/469 [==============================] - 41s 87ms/step - loss: 1.9137 - accuracy: 0.5712 - val_loss: 1.8388 - val_accuracy: 0.7117\n",
      "Epoch 9/50\n",
      "469/469 [==============================] - ETA: 0s - loss: 1.7970 - accuracy: 0.6143\n",
      "Epoch 9: val_accuracy improved from 0.71170 to 0.74020, saving model to .\\mnist.h5\n",
      "469/469 [==============================] - 41s 87ms/step - loss: 1.7970 - accuracy: 0.6143 - val_loss: 1.7057 - val_accuracy: 0.7402\n",
      "Epoch 10/50\n",
      "469/469 [==============================] - ETA: 0s - loss: 1.6615 - accuracy: 0.6499\n",
      "Epoch 10: val_accuracy improved from 0.74020 to 0.76320, saving model to .\\mnist.h5\n",
      "469/469 [==============================] - 41s 87ms/step - loss: 1.6615 - accuracy: 0.6499 - val_loss: 1.5536 - val_accuracy: 0.7632\n",
      "Epoch 11/50\n",
      "469/469 [==============================] - ETA: 0s - loss: 1.5137 - accuracy: 0.6787\n",
      "Epoch 11: val_accuracy improved from 0.76320 to 0.77930, saving model to .\\mnist.h5\n",
      "469/469 [==============================] - 41s 88ms/step - loss: 1.5137 - accuracy: 0.6787 - val_loss: 1.3924 - val_accuracy: 0.7793\n",
      "Epoch 12/50\n",
      "469/469 [==============================] - ETA: 0s - loss: 1.3653 - accuracy: 0.6980\n",
      "Epoch 12: val_accuracy improved from 0.77930 to 0.79050, saving model to .\\mnist.h5\n",
      "469/469 [==============================] - 42s 89ms/step - loss: 1.3653 - accuracy: 0.6980 - val_loss: 1.2367 - val_accuracy: 0.7905\n",
      "Epoch 13/50\n",
      "469/469 [==============================] - ETA: 0s - loss: 1.2309 - accuracy: 0.7107\n",
      "Epoch 13: val_accuracy improved from 0.79050 to 0.79630, saving model to .\\mnist.h5\n",
      "469/469 [==============================] - 43s 91ms/step - loss: 1.2309 - accuracy: 0.7107 - val_loss: 1.0986 - val_accuracy: 0.7963\n",
      "Epoch 14/50\n",
      "469/469 [==============================] - ETA: 0s - loss: 1.1142 - accuracy: 0.7246\n",
      "Epoch 14: val_accuracy improved from 0.79630 to 0.80330, saving model to .\\mnist.h5\n",
      "469/469 [==============================] - 42s 89ms/step - loss: 1.1142 - accuracy: 0.7246 - val_loss: 0.9839 - val_accuracy: 0.8033\n",
      "Epoch 15/50\n",
      "469/469 [==============================] - ETA: 0s - loss: 1.0201 - accuracy: 0.7309\n",
      "Epoch 15: val_accuracy improved from 0.80330 to 0.81000, saving model to .\\mnist.h5\n",
      "469/469 [==============================] - 42s 90ms/step - loss: 1.0201 - accuracy: 0.7309 - val_loss: 0.8919 - val_accuracy: 0.8100\n",
      "Epoch 16/50\n",
      "469/469 [==============================] - ETA: 0s - loss: 0.9449 - accuracy: 0.7426\n",
      "Epoch 16: val_accuracy improved from 0.81000 to 0.81670, saving model to .\\mnist.h5\n",
      "469/469 [==============================] - 42s 90ms/step - loss: 0.9449 - accuracy: 0.7426 - val_loss: 0.8188 - val_accuracy: 0.8167\n",
      "Epoch 17/50\n",
      "469/469 [==============================] - ETA: 0s - loss: 0.8872 - accuracy: 0.7495\n",
      "Epoch 17: val_accuracy improved from 0.81670 to 0.82130, saving model to .\\mnist.h5\n",
      "469/469 [==============================] - 41s 88ms/step - loss: 0.8872 - accuracy: 0.7495 - val_loss: 0.7607 - val_accuracy: 0.8213\n",
      "Epoch 18/50\n",
      "469/469 [==============================] - ETA: 0s - loss: 0.8390 - accuracy: 0.7578\n",
      "Epoch 18: val_accuracy improved from 0.82130 to 0.82590, saving model to .\\mnist.h5\n",
      "469/469 [==============================] - 41s 87ms/step - loss: 0.8390 - accuracy: 0.7578 - val_loss: 0.7139 - val_accuracy: 0.8259\n",
      "Epoch 19/50\n",
      "469/469 [==============================] - ETA: 0s - loss: 0.8006 - accuracy: 0.7612\n",
      "Epoch 19: val_accuracy improved from 0.82590 to 0.83080, saving model to .\\mnist.h5\n",
      "469/469 [==============================] - 41s 87ms/step - loss: 0.8006 - accuracy: 0.7612 - val_loss: 0.6756 - val_accuracy: 0.8308\n",
      "Epoch 20/50\n",
      "469/469 [==============================] - ETA: 0s - loss: 0.7709 - accuracy: 0.7680\n",
      "Epoch 20: val_accuracy improved from 0.83080 to 0.83500, saving model to .\\mnist.h5\n",
      "469/469 [==============================] - 41s 87ms/step - loss: 0.7709 - accuracy: 0.7680 - val_loss: 0.6439 - val_accuracy: 0.8350\n",
      "Epoch 21/50\n",
      "469/469 [==============================] - ETA: 0s - loss: 0.7449 - accuracy: 0.7737\n",
      "Epoch 21: val_accuracy improved from 0.83500 to 0.83950, saving model to .\\mnist.h5\n",
      "469/469 [==============================] - 41s 88ms/step - loss: 0.7449 - accuracy: 0.7737 - val_loss: 0.6173 - val_accuracy: 0.8395\n",
      "Epoch 22/50\n",
      "469/469 [==============================] - ETA: 0s - loss: 0.7215 - accuracy: 0.7779\n",
      "Epoch 22: val_accuracy improved from 0.83950 to 0.84250, saving model to .\\mnist.h5\n",
      "469/469 [==============================] - 41s 87ms/step - loss: 0.7215 - accuracy: 0.7779 - val_loss: 0.5944 - val_accuracy: 0.8425\n",
      "Epoch 23/50\n",
      "469/469 [==============================] - ETA: 0s - loss: 0.7001 - accuracy: 0.7837\n",
      "Epoch 23: val_accuracy improved from 0.84250 to 0.84650, saving model to .\\mnist.h5\n",
      "469/469 [==============================] - 41s 87ms/step - loss: 0.7001 - accuracy: 0.7837 - val_loss: 0.5747 - val_accuracy: 0.8465\n",
      "Epoch 24/50\n",
      "469/469 [==============================] - ETA: 0s - loss: 0.6816 - accuracy: 0.7879\n",
      "Epoch 24: val_accuracy improved from 0.84650 to 0.85090, saving model to .\\mnist.h5\n",
      "469/469 [==============================] - 47s 100ms/step - loss: 0.6816 - accuracy: 0.7879 - val_loss: 0.5575 - val_accuracy: 0.8509\n",
      "Epoch 25/50\n",
      "469/469 [==============================] - ETA: 0s - loss: 0.6685 - accuracy: 0.7909\n",
      "Epoch 25: val_accuracy improved from 0.85090 to 0.85260, saving model to .\\mnist.h5\n",
      "469/469 [==============================] - 43s 91ms/step - loss: 0.6685 - accuracy: 0.7909 - val_loss: 0.5424 - val_accuracy: 0.8526\n",
      "Epoch 26/50\n",
      "469/469 [==============================] - ETA: 0s - loss: 0.6548 - accuracy: 0.7949\n",
      "Epoch 26: val_accuracy improved from 0.85260 to 0.85550, saving model to .\\mnist.h5\n",
      "469/469 [==============================] - 40s 85ms/step - loss: 0.6548 - accuracy: 0.7949 - val_loss: 0.5291 - val_accuracy: 0.8555\n",
      "Epoch 27/50\n",
      "469/469 [==============================] - ETA: 0s - loss: 0.6426 - accuracy: 0.7980\n",
      "Epoch 27: val_accuracy improved from 0.85550 to 0.85730, saving model to .\\mnist.h5\n",
      "469/469 [==============================] - 42s 89ms/step - loss: 0.6426 - accuracy: 0.7980 - val_loss: 0.5171 - val_accuracy: 0.8573\n",
      "Epoch 28/50\n",
      "469/469 [==============================] - ETA: 0s - loss: 0.6321 - accuracy: 0.8002\n",
      "Epoch 28: val_accuracy improved from 0.85730 to 0.85980, saving model to .\\mnist.h5\n",
      "469/469 [==============================] - 38s 81ms/step - loss: 0.6321 - accuracy: 0.8002 - val_loss: 0.5062 - val_accuracy: 0.8598\n",
      "Epoch 29/50\n",
      "469/469 [==============================] - ETA: 0s - loss: 0.6202 - accuracy: 0.8040\n",
      "Epoch 29: val_accuracy improved from 0.85980 to 0.86240, saving model to .\\mnist.h5\n",
      "469/469 [==============================] - 39s 84ms/step - loss: 0.6202 - accuracy: 0.8040 - val_loss: 0.4963 - val_accuracy: 0.8624\n",
      "Epoch 29: early stopping\n",
      "The model has successfully trained\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split= 0.3, callbacks= cb, validation_data=(X_test, y_test))\n",
    "print(\"The model has successfully trained\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e06ff7da33dc9620448857a90ad8b5f428f0d573d205a934d2841c8aee45ea32"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
